import json 
import numpy as np
import fileinput
import re 

# Victoria Hong
# Parser.py handles input from go-audit.
# Used as library, can be run for demo purposes
# User Defined Inputs:
#   Batch Size
#   Audit Data Source:
#       - Syslog file generated by go audit
#       - Batch of batchSize from Stream
#   Properties to analyze:
#       - User defines list of properties to collect
#           - lists should map to same order of wanted properties
#           i.e. wanted = ['pid', 'ppid', 'timestamp', 'syscall', 'uid']
#                  formats input in list of same order [5, 7, 1573692264.585, 0]
#
# To collect raw data: processFile(non_malicious_fp, allFields)
#   
#
# LIBRARY FUNCTIONS  
#
# dataParse (String line, list<String> wants)
#   map wants list to the values extracted from data
#   can be used to parse batches from files or streams
#       i.e. for line in batch: dataParse(line, ['pid'])
#       will parse pids from all lines in the batch via for loop
#
# countSort (batches (output from processbatch/file), int column, list<int> range_):
#       batches = results from processFile or batches made from dataParse
#       gives a 2d array given a property and the count of the property within the batch\
#           i.e. wanted = ['pid', 'ppid', 'timestamp', 'syscall', 'uid']
#           if column == 1, counts unique pid occurences.
#           who wants to address columns as 0 based if they aren't coding.... 
#
# processFile (String fpath_, list<String> wants):
#       loads lines from file and truncates to wanted properties
#       splits lines into batches of batchSize
#
# processStreamBatch (list<JSON> rabbitInput)
#       rabbitMQ can send a list of JSON objects to process batch




#
# Library Functions

# Default properties to collect:  all present fields in data
allFields = ["arch", " syscall", " success", " exit", " a0", " a1", " a2", " a3", " items", " ppid", " pid", " auid", " uid", " gid", " euid", " suid", " fsuid", " egid", " sgid", " fsgid", " tty", " ses", " comm", " exe", " subj", " key"]

batchSize = 64 # size of each batch

rx = re.compile("audit.(?P<timestamp>.*):(?P<sequence>.*).: arch=(?P<arch>.*) syscall=(?P<syscall>\d+) (?:success=(?P<success>.*) exit=(?P<exit>.*))?[ ]*a0=(?P<a0>.*) a1=(?P<a1>.*) a2=(?P<a2>.*) a3=(?P<a3>.*) items=(?P<items>.*) ppid=(?P<ppid>.*) pid=(?P<pid>.*) auid=(?P<auid>.*) uid=(?P<uid>.*) gid=(?P<gid>.*) euid=(?P<euid>.*) suid=(?P<suid>.*) fsuid=(?P<fsuid>.*) egid=(?P<egid>.*) sgid=(?P<sgid>.*) fsgid=(?P<fsgid>.*) tty=(?P<tty>.*) ses=(?P<ses>.*) comm=(?P<comm>.*) exe=(?P<exe>.*)[ ]*(?:subj=(?P<subj>.*))? key=(?P<key>.*)")
#list of all current processes to collect
monitoredComms = []

def dataParseFile( line, wants):
    # make sure data is properly formatted
    output = {key: None for key in wants}
    data = [x.split("=") for x in line['messages'][0]['data'].split()]
    data.append(['timestamp', line['timestamp']])
    for x in data:
        key, value = x[0], x[1]
        if key in wants:
            output[key] = value

    return list(output.values())



    

# line = list of dictionaries
def dataParseStream(batches, wants):
    agg = []
    for batch in batches:
        agg.append([batch[x] for x in wants])
    return agg


def countSort( batches, column, range_):
    # if range input is range(0,n), construct a list
    aggregate = []
    count = [0] * len(list(range_))

    for batch in batches:
        for element in batch:
            count[int(element[column-1])] += 1
        # add batch statistics to result and reset for next batch
        aggregate.append(count)
        count = [0] * len(list(range_))
    return aggregate

#def postProcessByComm( batches):



def processFile(fpath_, wants):
    lineNumber = 0
    batches = []
    f = open(fpath_).readlines()
    batch = []

    for line in f:
        line = json.loads(line)

        data = dataParseFile(line, wants)
        if len(data) != len(wants):
            # TODO: exit or skip improperly formatted data?
            print("Missing property found at {}:\n{}", lineNumber, line)
            #exit(-1)  # shouldn't read corrupt data, exit
        else:
            lineNumber += 1
            batch.append(data)
            if lineNumber % batchSize == 0:
                batches.append(batch)
                batch = []

    if len(batch) > 0:
        batches.append(batch)

    return batches



# TODO: NEEDS TESTING
def processStreamBatch(rabbitInput):
    if len(rabbitInput) != batchSize:
        print("batch size is {} but needs to be {}", len(rabbitInput), batchSize)
    else:
        return [dataParse(line) for line in rabbitInput]


def dataParseRaw(path):
    f = open(path,"r")
    wtf = f.readlines()

    for line in wtf:
        if line[5:12] == "SYSCALL":
            result = re.search(rx,line)
            print(result.groupdict())
            if len(data) != len(wants):
                # TODO: exit or skip improperly formatted data?
                print("Missing property found at {}:\n{}", lineNumber, line)
                #exit(-1)  # shouldn't read corrupt data, exit
            else:
                lineNumber += 1
                batch.append(data)
                if lineNumber % batchSize == 0:
                    batches.append(batch)
                    batch = []

    if len(batch) > 0:
        batches.append(batch)

    return batches


def appendExeFiles(mypath):
    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]
    onlyfiles.sort()
    print(onlyfiles)



PATH = "/home/carla/Desktop/mycpy/vboxtest/vBoxTest/audit/lol.py"
appendExeFiles(PATH)
#dataParseRaw(PATH)



